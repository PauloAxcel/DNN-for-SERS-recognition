{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan  9 23:22:07 2020\n",
    "\n",
    "@author: paulo\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.applications import VGG16\n",
    "from vis.losses import ActivationMaximization\n",
    "from vis.regularizers import TotalVariation, LPNorm\n",
    "from vis.input_modifiers import Jitter\n",
    "from vis.optimizer import Optimizer\n",
    "from vis.callbacks import GifGenerator\n",
    "import cv2\n",
    "from vis.utils import utils\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *=0.1\n",
    "    \n",
    "    x +=0.5\n",
    "    x = np.clip(x,0,1)\n",
    "    \n",
    "    x *= 255\n",
    "    x = np.clip(x , 0 , 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def generate_pattern(layer_name, filter_index, sizex = 128 , sizey = 141):\n",
    "    \n",
    "    layer_output = model.layers[0].get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:,:,:, filter_index])\n",
    "    \n",
    "    grads = K.gradients(loss, model.layers[0].layers[0].input)[0]\n",
    "    \n",
    "    grads /= (K.sqrt(K.mean(K.square(grads)))+ 1e-5)\n",
    "    \n",
    "    iterate = K.function([model.layers[0].layers[0].input], [loss, grads])\n",
    "    \n",
    "    input_img_data = np.random.random((1,sizex,sizey,3))*20+128\n",
    "    \n",
    "    step = 1\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "    \n",
    "    img = input_img_data[0]\n",
    "    \n",
    "    return deprocess_image(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 3 classes.\n",
      "Found 0 images belonging to 3 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#DEFINE PATHS FOR SAMPLES WITH DATA AUGMENTATION IN\n",
    "\n",
    "train_path = 'C:\\\\Users\\\\Paulo\\\\Desktop\\\\afm different colours\\\\keras_imaging\\\\data augmentation with original data\\\\train\\\\'\n",
    "test_path = 'C:\\\\Users\\\\Paulo\\\\Desktop\\\\afm different colours\\\\keras_imaging\\\\prediction2\\\\height_change\\\\'\n",
    "valid_path ='C:\\\\Users\\\\Paulo\\\\Desktop\\\\afm different colours\\\\keras_imaging\\\\data augmentation with original data\\\\validation\\\\'\n",
    "\n",
    "train_batch = ImageDataGenerator(rescale=1/.255).flow_from_directory(train_path, target_size=(128,141),\n",
    "                                 classes=['NOENH','SLIENH','SERS'], batch_size = 32)\n",
    "valid_batch = ImageDataGenerator(rescale=1/.255).flow_from_directory(valid_path, target_size=(128,141),\n",
    "                                 classes=['NOENH','SLIENH','SERS'], batch_size = 32)\n",
    "\n",
    "#USE PRETRAINED CNN REMOVE BOTTOM PART\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top = False, input_shape=(128,141,3))\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False\n",
    "classifier.add(conv_base)\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Flatten()) \n",
    "classifier.add(Dense(256, activation = 'relu'))\n",
    "classifier.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "#TRAIN THE CONVNET FOR THE OUTPUT LAYER\n",
    "\n",
    "classifier.compile(optimizer = 'rmsprop',\n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", \n",
    "                               monitor = 'val_accuracy',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "history = classifier.fit_generator(train_batch,\n",
    "                                   steps_per_epoch = 100,\n",
    "                                   epochs = 20,\n",
    "                                   callbacks=[checkpointer],\n",
    "                                   validation_data = valid_batch,\n",
    "                                   validation_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAYER FINE TUNE OF THE LAST CONVNET BLOCK, THE LAST BLOCK TAKES CARE OF CHARACTERIZING CLASS DIFFERENCES\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "from keras import optimizers        \n",
    "        \n",
    "classifier.compile(optimizer = optimizers.RMSprop(lr=1e-5),\n",
    "                   loss = 'categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])\n",
    "      \n",
    "  \n",
    "history = classifier.fit_generator(train_batch,\n",
    "                                   steps_per_epoch = 100,\n",
    "                                   epochs = 20,\n",
    "                                   validation_data = valid_batch,\n",
    "                                   validation_steps = 50)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK IF THERE IS OVERFITTING BY DRAWING THE ACC AND LOSS\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs,val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs,loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for layer in classifier.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "file = os.listdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT TEST IMAGES\n",
    "\n",
    "labels = ['No Enhancement','Slightly Enhancement' , 'SERS']\n",
    "\n",
    "\n",
    "for i in range(len(file)):\n",
    "    img = image.load_img(test_path+file[i], target_size=(128,141))\n",
    "    img_tensor = image.img_to_array(img)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "    img_tensor /= 255.\n",
    "    \n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    images = np.vstack([x])\n",
    "    \n",
    "    preds = model.predict(images)\n",
    "    \n",
    "    color = []\n",
    "    \n",
    "    if np.argmax(preds) == 0:\n",
    "        color.append(['r','k','k'])\n",
    "    if np.argmax(preds) == 1:\n",
    "        color.append(['k','r','k'])\n",
    "    if np.argmax(preds) == 2:\n",
    "        color.append(['k','k','r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output from the conv net and not from the pooling...        \n",
    "\n",
    "    img_output = model.layers[0].layers[-2].output[:,np.argmax(preds)]\n",
    "    last_conv_layer = model.layers[0].get_layer('block5_conv3')\n",
    "    \n",
    "    grads = K.gradients(img_output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis= (0,1,2))\n",
    "    \n",
    "    iterate = K.function([ model.layers[0].layers[0].input],\n",
    "                         [pooled_grads, last_conv_layer.output[0]])\n",
    "    \n",
    "    pooled_grads_value , conv_layer_output_value = iterate([x])\n",
    "    \n",
    "    for j in range(512):\n",
    "        conv_layer_output_value[:,:,j] *= pooled_grads_value[j]\n",
    "        \n",
    "    heatmap = np.mean(conv_layer_output_value , axis=-1)\n",
    "    heatmap = np.maximum(heatmap,0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    \n",
    "    img = cv2.imread(test_path+file[i])\n",
    "    \n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1],img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_HOT)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "#    fig ,(ax1,ax2) = plt.subplots(1,2)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.axis('off')\n",
    "\n",
    "    img *= np.uint8(255.0/img.max())\n",
    "\n",
    "    blend = cv2.addWeighted(img,0.5, heatmap,0.5, 0)\n",
    "    \n",
    "    im = ax.imshow(blend,interpolation='lanczos',cmap='hot')\n",
    "    \n",
    "    x = np.linspace(0,blend.shape[1], blend.shape[1])\n",
    "    y = np.linspace(0, blend.shape[0], blend.shape[0])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    contour = ax.contour(X,Y,rgb2gray(heatmap),3, colors = 'black')\n",
    "    ax.clabel(contour, inline=True, fontsize=18)\n",
    "\n",
    "    \n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)   \n",
    "    \n",
    "    \n",
    "    cbar = fig.colorbar(im, cax=cax, ticks=[blend.min(), np.mean([blend.max(),blend.min()]), blend.max()], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['Low', 'Medium', 'High'], fontdict={'fontsize': 18, 'fontweight': 'medium'})  # horizontal colorbar\n",
    "    \n",
    "    ax.text(img.shape[1]+12.5,-2,'Activation', fontdict={'fontsize': 24, 'fontweight': 'medium'})\n",
    "    ax.text(0 , -2, labels[0] +  \" %.1f\" % (preds[0][0]*100) ,fontdict={'fontsize': 18, 'fontweight': 'medium'} , color=color[0][0])\n",
    "    ax.text(img.shape[1]*2//5 , -2, labels[1] + \" %.1f\" % (preds[0][1]*100), fontdict={'fontsize': 18, 'fontweight': 'medium'} , color=color[0][1])\n",
    "    ax.text(img.shape[1]*4//5 , -2, labels[2] + \" %.1f\" % (preds[0][2]*100), fontdict={'fontsize': 18, 'fontweight': 'medium'} , color=color[0][2])\n",
    "    \n",
    "    ax.text(img.shape[0]//2,-8,file[i],fontdict={'fontsize': 24, 'fontweight': 'medium'})\n",
    "    \n",
    "    fig.savefig('C:\\\\Users\\\\Paulo\\\\Desktop\\\\afm different colours\\\\DNN\\\\'+file[i]+'.png', dpi = 300,bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
